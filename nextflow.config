/*
========================================================================================
    ccsmethphase Nextflow config file
========================================================================================
    Default config options for all compute environments
----------------------------------------------------------------------------------------
    learn from nf-core and nanome
*/

// Global default params, used in configs
params {
    help                       = false

    conda_base_dir             = null  // sample: /opt/conda
    conda_name                 = null  // sample: opt/conda/envs/ccsmethphase
    conda_cache                = 'local_conda_cache'

    docker_name                = "nipengcsu/ccsmethphase:latest"
    singularity_name           = "docker://nipengcsu/ccsmethphase:latest"
    singularity_cache          = 'local_singularity_cache'

    clair3_conda_name          = null
    clair3_docker_name         = "hkubal/clair3:latest"
    clair3_singularity_name    = "docker://hkubal/clair3:latest"

    gpu                        = false
    containerOptions           = null // or "--gpus all" for docker

    // Specify your pipeline's command line flags
    // Input options ===============
    input                      = null // input_sheet.tsv
    dsname                     = "test"
    outdir                     = "ccsmethphase_results"

    // References
    genome                     = "GRCh38" // could use igenomes["GRch38"]?
    igenomes_base              = 's3://ngi-igenomes/igenomes'
    igenomes_ignore            = false
    include_all_ctgs           = false  // default false, means only use contig chr{1..22,X,Y} and {1..22,X,Y}
                                        // effective in clair3,

    genome_dir                 = "genome_ref_dir"
    genome_file                = dsname+".genome_ref.fasta"

    // for the tools of the pipeline ===============
    // run_call_hifi    = true  // abandoned

    // ccsmeth
    DEFAULT_CCSMETH_CM_MODEL     = "/opt/models/ccsmeth/model_ccsmeth_5mCpG_call_mods_attbigru2s_b21.v1.ckpt"
    DEFAULT_CCSMETH_AG_MODEL     = "/opt/models/ccsmeth/model_ccsmeth_5mCpG_aggregate_attbigru_b11.v2.ckpt"
    // - call_mods
    run_call_mods                = true
    ccsmeth_cm_model             = DEFAULT_CCSMETH_CM_MODEL  // call_mods_model
    cm_nproc_gpu                 = 6
    // - call_freq
    run_call_freq                = true
    cf_mode                      = "aggregate" // ["count", "aggregate"]
    ccsmeth_ag_model             = DEFAULT_CCSMETH_AG_MODEL // aggregate_model
    cf_always_count              = false
    cf_bonus_options             = '' // '--refsites_all --mapq 0 --identity 0'

    run_align                    = true
    aligner                      = "pbmm2"  // ["pbmm2", "minimap2"]

    // postalign_combine            = false  // abandoned

    run_clair3                   = true
    clair3_hifi_model            = "/opt/models/hifi"

    run_whatshap                 = true  // run phase and haplotag of whatshap

    dataType = null
    chrSet = null

    eval_methcall     = false
    bs_bedmethyl      = null  // bs bedmethyl file as standard for evaluation
    comb_strands      = true  // combine CpG methylation freq using reads aligned to both strands
    eval_fwd_only     = true  // only cmp fwd CpGs
    
    cleanup           = false // If clean work dir after complete
    // ===================================================================================

    enable_conda                 = false
    use_singularity              = false
    use_docker                   = false

    // Max resource options
    // Defaults only, expecting to be overwritten
    max_memory                   = '200.GB'
    max_cpus                     = 40
    max_time                     = '360.h'

    tracedir                     = "${params.outdir}/pipeline_info"

}

// Load modules.config for DSL2 module specific options
// includeConfig 'conf/modules.config'

// Load igenomes.config if required
if (!params.igenomes_ignore) {
    includeConfig 'conf/igenomes.config'
} else {
    params.genomes = [:]
}

profiles {
    debug { process.beforeScript = 'echo $HOSTNAME' }
    conda {
        params.enable_conda    = true
        docker.enabled         = false
        singularity.enabled    = false
        podman.enabled         = false
        shifter.enabled        = false
        charliecloud.enabled   = false
        // process.conda = params.conda_name
        conda.cacheDir = params.conda_cache
    }
    docker {
        params {
            use_docker       = true
            containerOptions = null // users using GPU need to set to "--gpus all"
        }
        // process.container      = params.docker_name
        process.containerOptions = params.containerOptions // or "--gpus all" Note: this is not compatible with GitHub citest/naive docker users
        docker.envWhitelist = 'CUDA_VISIBLE_DEVICES'
        docker.enabled         = true
        docker.userEmulation   = true
        singularity.enabled    = false
        podman.enabled         = false
        shifter.enabled        = false
        charliecloud.enabled   = false
    }
    singularity {
        params {
            use_singularity  = true
            containerOptions = "--nv"
        }
        process {
            // container = params.singularity_name
            containerOptions = params.containerOptions // "--nv"
        }
        singularity.cacheDir = params.singularity_cache
        singularity.envWhitelist = 'CUDA_VISIBLE_DEVICES' // Ref: https://github.com/nextflow-io/nextflow/issues/776
        singularity.enabled    = true
        singularity.autoMounts = true
        docker.enabled         = false
        podman.enabled         = false
        shifter.enabled        = false
        charliecloud.enabled   = false
    }
    // hpc with singularity-loading, refer from nanome
    hpc_sing { // general hpc configuration
		params {
			// hpc slurm default parameters
			qos = 'cpuq'
			partition = 'cpuQ'
			queue = partition
			processors = 40
			memory = '200.GB'
			time = '50.h'
			gresOptions = null // 'gpu:1'
			account = 'pi_zy'

			// Defaults max resource
			max_memory                 = 200.GB
			max_cpus                   = 40
			max_time                   = 360.h

			queueSize = 10	// max number of job submit
		}
		process {
			executor = 'slurm'

			queue = params.queue
			qos = params.qos
			cpus = params.processors
			memory = params.memory
			time = params.time
			clusterOptions = "-A ${params.account} -q ${params.qos} -p ${params.partition} ${params.gresOptions ? '--gres=' + params.gresOptions : ' '}"

			beforeScript = 'module load singularity'  // this is for combining singularity with hpc
		}
		executor {
			queueSize = params.queueSize
			submitRateLimit = '10/1min'
			exitReadTimeout = '500 sec'
			pollInterval = '60 sec'
		}
	}
    podman {
        podman.enabled         = true
        docker.enabled         = false
        singularity.enabled    = false
        shifter.enabled        = false
        charliecloud.enabled   = false
    }
    shifter {
        shifter.enabled        = true
        docker.enabled         = false
        singularity.enabled    = false
        podman.enabled         = false
        charliecloud.enabled   = false
    }
    charliecloud {
        charliecloud.enabled   = true
        docker.enabled         = false
        singularity.enabled    = false
        podman.enabled         = false
        shifter.enabled        = false
    }
    test      { includeConfig 'conf/test.config'      }
}

// Load base.config by default for all pipelines
includeConfig 'conf/base.config'

// Export these variables to prevent local Python/R libraries from conflicting with those in the container
env {
    PYTHONNOUSERSITE = 1
    R_PROFILE_USER   = "/.Rprofile"
    R_ENVIRON_USER   = "/.Renviron"
}

// Capture exit codes from upstream processes when piping
process.shell = ['/bin/bash', '-euo', 'pipefail']

def trace_timestamp = new java.util.Date().format( 'yyyy-MM-dd_HH-mm-ss')
timeline {
    enabled = true
    file    = "${params.tracedir}/execution_timeline_${trace_timestamp}.html"
}
report {
    enabled = true
    file    = "${params.tracedir}/execution_report_${trace_timestamp}.html"
}
trace {
    enabled = true
    file    = "${params.tracedir}/execution_trace_${trace_timestamp}.txt"
}
dag {
    enabled = true
    file    = "${params.tracedir}/pipeline_dag_${trace_timestamp}.svg"
}

manifest {
    name            = 'ccsmethphase'
    author          = 'Peng Ni'
    homePage        = 'https://github.com/PengNi/ccsmethphase'
    description     = 'methylation phasing using pacbio ccs reads'
    mainScript      = 'main.nf'
    nextflowVersion = '!>=21.04.0'
    version         = '1.0dev'
}

// Function to ensure that resource requirements don't go beyond
// a maximum limit
def check_max(obj, type) {
    if (type == 'memory') {
        try {
            if (obj.compareTo(params.max_memory as nextflow.util.MemoryUnit) == 1)
                return params.max_memory as nextflow.util.MemoryUnit
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max memory '${params.max_memory}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'time') {
        try {
            if (obj.compareTo(params.max_time as nextflow.util.Duration) == 1)
                return params.max_time as nextflow.util.Duration
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max time '${params.max_time}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'cpus') {
        try {
            return Math.min( obj, params.max_cpus as int )
        } catch (all) {
            println "   ### ERROR ###   Max cpus '${params.max_cpus}' is not valid! Using default value: $obj"
            return obj
        }
    }
}
